{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Lab\n",
    "In this lab, you will build a deep learning network that locates a particular human target within an image.  The premise is that a quadcopter (simulated) is searching for a target, and then will follow the target once found.  It's not enough to simply say the target is present in the image in this case, but rather to know *where* in the image the target is, so that the copter can adjust its direction in order to follow.\n",
    "\n",
    "Consequently, an image classification network is not enough to solve the problem. Intead, a semantic segmentation network is needed so that the target can be specifically located within the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "We have provided you with the dataset for this lab. If you haven't already downloaded the training and validation datasets, you can check out the README for this lab's repo for instructions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.keras.python import keras\n",
    "from tensorflow.contrib.keras.python.keras import layers, models\n",
    "\n",
    "from tensorflow import image\n",
    "\n",
    "from utils import scoring_utils\n",
    "from utils.separable_conv2d import SeparableConv2DKeras, BilinearUpSampling2D\n",
    "from utils import data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.2'), 'Please use TensorFlow version 1.2 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Layers\n",
    "In the Classroom, we discussed the different layers that constitute a fully convolutional network. The following code will intoduce you to the functions that you will be using to build out your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Convolutions\n",
    "The Encoder for your FCN will essentially require separable convolution layers. Below we have implemented two functions - one which you can call upon to build out separable convolutions or regular convolutions. Each with batch normalization and with the ReLU activation function applied to the layers. \n",
    "\n",
    "While we recommend the use of separable convolutions thanks to their advantages we covered in the Classroom, some of the helper code we will present for your model will require the use for regular convolutions. But we encourage you to try and experiment with each as well!\n",
    "\n",
    "The following will help you create the encoder block and the final model for your architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separable_conv2d_batchnorm(input_layer, filters, strides=1):\n",
    "    output_layer = SeparableConv2DKeras(filters=filters,kernel_size=3, strides=strides,\n",
    "                             padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "    output_layer = layers.BatchNormalization()(output_layer) \n",
    "    return output_layer\n",
    "\n",
    "def conv2d_batchnorm(input_layer, filters, kernel_size=3, strides=1):\n",
    "    output_layer = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=1, \n",
    "                      padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "    output_layer = layers.BatchNormalization()(output_layer) \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Upsampling\n",
    "The following helper function will help implement the bilinear upsampling layer. Currently, upsampling by a factor of 2 is recommended but you can try out different factors as well. You will use this to create the decoder block later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bilinear_upsample(input_layer):\n",
    "    output_layer = BilinearUpSampling2D((2,2))(input_layer)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "In the following cells, we will cover how to build the model for the task at hand. \n",
    "\n",
    "- We will first create an Encoder Block, where you will create a separable convolution layer using an input layer and the size(depth) of the filters as your inputs.\n",
    "- Next, you will create the Decoder Block, where you will create an upsampling layer using bilinear upsampling, followed by a layer concatentaion, and some separable convolution layers.\n",
    "- Finally, you will combine the above two and create the model. In this step you will be able to experiment with different number of layers and filter sizes for each to build your model.\n",
    "\n",
    "Let's cover them individually below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Block\n",
    "Below you will create a separable convolution layer using the separable_conv2d_batchnorm() function. The `filters` parameter defines the size or depth of the output layer. For example, 32 or 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_block(input_layer, filters, strides):\n",
    "    \n",
    "    # TODO Create a separable convolution layer using the separable_conv2d_batchnorm() function.\n",
    "    output_layer = separable_conv2d_batchnorm(input_layer, filters, strides)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Block\n",
    "The decoder block, as covered in the Classroom, comprises of three steps -\n",
    "\n",
    "- A bilinear upsampling layer using the upsample_bilinear() function. The current recommended factor for upsampling is set to 2.\n",
    "- A layer concatenation step. This step is similar to skip connections. You will concatenate the upsampled small_ip_layer and the large_ip_layer.\n",
    "- Some (one or two) additional separable convolution layers to extract some more spatial information from prior layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_block(small_ip_layer, large_ip_layer, filters):\n",
    "    # TODO Upsample the small input layer using the bilinear_upsample() function.\n",
    "    upsampled_layer = bilinear_upsample(small_ip_layer)\n",
    "    \n",
    "    # TODO Concatenate the upsampled and large input layers using layers.concatenate\n",
    "    output_layer = layers.concatenate([upsampled_layer, large_ip_layer])\n",
    "    \n",
    "    # TODO Add some number of separable convolution layers\n",
    "    output_layer = separable_conv2d_batchnorm(output_layer, filters)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Now that you have the encoder and decoder blocks ready, you can go ahead and build your model architecture! \n",
    "\n",
    "There are three steps to the following:\n",
    "- Add encoder blocks to build out initial set of layers. This is similar to how you added regular convolutional layers in your CNN lab.\n",
    "- Add 1x1 Convolution layer using conv2d_batchnorm() function. Remember that 1x1 Convolutions require a kernel and stride of 1.\n",
    "- Add decoder blocks for upsampling and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn_model(inputs, num_classes):\n",
    "    \n",
    "    # TODO Add Encoder Blocks. \n",
    "    # Remember that with each encoder layer, the depth of your model (the number of filters) increases.\n",
    "    e0 = encoder_block(inputs, 16, 2)\n",
    "    e1 = encoder_block(e0, 32, 2)\n",
    "    e2 = encoder_block(e1, 64, 2)\n",
    "    e3 = encoder_block(e2, 128, 2)\n",
    "    \n",
    "    # TODO Add 1x1 Convolution layer using conv2d_batchnorm().\n",
    "    bn = conv2d_batchnorm(e3,128,kernel_size=1)\n",
    "    \n",
    "    # TODO: Add the same number of Decoder Blocks as the number of Encoder Blocks\n",
    "    d1 = decoder_block(bn, e2, 128)\n",
    "    d2 = decoder_block(d1, e1, 64)\n",
    "    d3 = decoder_block(d2, e0,32 )\n",
    "    x = decoder_block(d3, inputs, 16)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The function returns the output layer of your model. \"x\" is the final layer obtained from the last decoder_block()\n",
    "    return layers.Conv2D(num_classes, 3, activation='softmax', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The following cells will utilize the model you created and define an ouput layer based on the input and the number of classes.Following that you will define the hyperparameters to compile and train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "image_hw = 128\n",
    "image_shape = (image_hw, image_hw, 3)\n",
    "inputs = layers.Input(image_shape)\n",
    "num_classes = 3\n",
    "\n",
    "# Call fcn_model()\n",
    "output_layer = fcn_model(inputs, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "Below you can define and tune your hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - 445s - loss: 0.1855 - val_loss: 0.0721\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 440s - loss: 0.0278 - val_loss: 0.0404\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 441s - loss: 0.0226 - val_loss: 0.0395\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 441s - loss: 0.0203 - val_loss: 0.0399\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 441s - loss: 0.0190 - val_loss: 0.0370\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 441s - loss: 0.0190 - val_loss: 0.0345\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 441s - loss: 0.0198 - val_loss: 0.0343\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 443s - loss: 0.0173 - val_loss: 0.0340\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 441s - loss: 0.0170 - val_loss: 0.0331\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 440s - loss: 0.0158 - val_loss: 0.0343\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 442s - loss: 0.0151 - val_loss: 0.0352\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 443s - loss: 0.0148 - val_loss: 0.0324\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 441s - loss: 0.0151 - val_loss: 0.0327\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 441s - loss: 0.0156 - val_loss: 0.0488\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 443s - loss: 0.0149 - val_loss: 0.0333\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 441s - loss: 0.0154 - val_loss: 0.0326\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 442s - loss: 0.0146 - val_loss: 0.0327\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 440s - loss: 0.0142 - val_loss: 0.0339\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 441s - loss: 0.0137 - val_loss: 0.0303\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 442s - loss: 0.0136 - val_loss: 0.0327\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 441s - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 440s - loss: 0.0131 - val_loss: 0.0331\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 441s - loss: 0.0134 - val_loss: 0.0327\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 440s - loss: 0.0135 - val_loss: 0.0329\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 440s - loss: 0.0129 - val_loss: 0.0333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f8fe4e48cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Define the Keras model and compile it for training\n",
    "model = models.Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='categorical_crossentropy')\n",
    "\n",
    "# Data iterators for loading the training and validation data\n",
    "train_iter = data_iterator.BatchIteratorSimple(batch_size=batch_size,\n",
    "                                               data_folder=os.path.join('..', 'data', 'train'),\n",
    "                                               image_shape=image_shape,\n",
    "                                               shift_aug=True)\n",
    "\n",
    "val_iter = data_iterator.BatchIteratorSimple(batch_size=batch_size,\n",
    "                                             data_folder=os.path.join('..', 'data', 'validation'),\n",
    "                                             image_shape=image_shape)\n",
    "\n",
    "model.fit_generator(train_iter,\n",
    "                    steps_per_epoch = 500, # the number of batches per epoch,\n",
    "                    epochs = num_epochs, # the number of epochs to train for,\n",
    "                    validation_data = val_iter, # validation iterator\n",
    "                    validation_steps = 50, # the number of batches to validate on\n",
    "                    workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save your trained model weights\n",
    "weight_file_name = 'model_weights_big_network'\n",
    "model.save_weights(os.path.join('..', 'data', 'weights', weight_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you need to load a model which you previously trained you can uncomment the codeline that calls the following function.\n",
    "def load_weights(your_model, your_weight_filename):\n",
    "    model_path = os.path.join('..', 'data', 'weights', your_weight_filename)\n",
    "    if os.path.exists(model_path):\n",
    "        model = your_model.load_weights(model_path)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError('No weight file found at {}'.format(model_path))\n",
    "\n",
    "# model = load_weights(model, weight_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir_if_not_exist(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE only modify these lines if you have changed where data is being stored(not recommended)\n",
    "validation_path = os.path.join('..', 'data', 'validation')\n",
    "file_names = sorted(glob.glob(os.path.join(validation_path, 'images', '*.jpeg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = 'prediction'# TODO add the name of folder to save these predictions to\n",
    "output_path = os.path.join('..', 'data', 'runs', experiment_name)\n",
    "make_dir_if_not_exist(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in file_names:\n",
    "    image = misc.imread(name)\n",
    "    if image.shape[0] != image_shape[0]:\n",
    "         image = misc.imresize(image,image_shape)\n",
    "    image = data_iterator.preprocess_input(image.astype(np.float32))\n",
    "    pred = model.predict_on_batch(np.expand_dims(image, 0))\n",
    "    base_name = os.path.basename(name).split('.')[0]\n",
    "    base_name = base_name + '_prediction.png'\n",
    "    misc.imsave(os.path.join(output_path, base_name), np.squeeze((pred * 255).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's evaluate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of validation samples intersection over the union evaulated on 1184\n",
      "average intersection over union for background is 0.9932846495153487\n",
      "average intersection over union for hero is 0.4130629056037957\n",
      "average intersection over union for other people is 0.16145285632529788\n",
      "global average intersection over union is 0.5226001371481475\n"
     ]
    }
   ],
   "source": [
    "scoring_utils.score_run(validation_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
